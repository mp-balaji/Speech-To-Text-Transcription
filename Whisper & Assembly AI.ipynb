{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5650cf66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\admin\\appdata\\local\\temp\\pip-req-build-iqe0qi3l\n",
      "  Resolved https://github.com/openai/whisper.git to commit 7858aa9c08d98f75575035ecd6481f462d66ca27\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai-whisper==20230124) (1.20.3)\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai-whisper==20230124) (4.62.3)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openai-whisper==20230124) (8.10.0)\n",
      "Collecting transformers>=4.19.0\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting ffmpeg-python==0.2.0\n",
      "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in c:\\users\\admin\\anaconda3\\lib\\site-packages (from ffmpeg-python==0.2.0->openai-whisper==20230124) (0.18.2)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (2021.8.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers>=4.19.0->openai-whisper==20230124) (2.26.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.19.0->openai-whisper==20230124) (4.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers>=4.19.0->openai-whisper==20230124) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->openai-whisper==20230124) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers>=4.19.0->openai-whisper==20230124) (1.26.7)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (setup.py): started\n",
      "  Building wheel for openai-whisper (setup.py): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20230124-py3-none-any.whl size=1191302 sha256=99ecdb48b60bcb93a9f1f83cfabfc2a8df499054a1a5bf7bd4d46f4115f541b5\n",
      "  Stored in directory: C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-xim6gc2n\\wheels\\fe\\03\\29\\e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, torch, ffmpeg-python, openai-whisper\n",
      "Successfully installed ffmpeg-python-0.2.0 huggingface-hub-0.12.0 openai-whisper-20230124 tokenizers-0.13.2 torch-1.13.1 transformers-4.26.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/openai/whisper.git 'C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-req-build-iqe0qi3l'\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3823e273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:12<00:00, 11.7MiB/s]\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\whisper\\transcribe.py:79: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay. So we'll just give it another minute. Hi, Balaji. Hi, Dr. How are you? I'm doing good. How are you? Glad to have you on the call. Yeah, I like almost forgot. Thanks for the thing. Even I. I was searching for the link. I'm glad you made it. We could get started and then whoever joins joins because I'm doing the recording again. So I'm going to share my screen and I guess we'll get started. Can you guys see the screen? Yeah. Yeah. So we'll start with this. This is where we got with the. The articles. We've got a good amount of basic articles to use here. And we've got here. I put some notes for some of the articles that I got just generally what. What we would need that paper for, you know, if we're referencing the eye gazing or brain imaging or whatever. The weights here also which is going to help us to decide when to access these articles. So we'll say that this phase is now done. This is our dumping phase and the dumping phase is. Getting the contents together in one place. And now what we're going to do is in two parts. First part I'm going to show you here on the screen. This is going to be like the template document that we'll start working with. I haven't shared this with anyone yet because I'm going to break each section up and we're going to give it to different people. So we'll start, not with, there's going to be all of these, you know, statements and declarations. We'll fill that out later. So what we've got here is an outline of the beginning of the structure of our paper. Somebody will be in charge of abbreviation. Somebody else will be in charge of, you know, a different section. So today what I'm hoping to do is to. Based on whoever is here today to become sort of a sub leader of a different section. For example, somebody will be a sub leader for the data section and that means you and others under you will be communicating with the data collection team and the modeling team. So I'm going to get information that will then complete this section things about the repository, the format of the data was it in CSV was it in multiple columns, what were the categories, what were the features, etc. I will break this particular section up and give it to whoever volunteers under hypothesis objectives and methodology that will be another team, the exploratory data analysis and feature engineering. This person will be communicating with the guys who are running the models, the ML models. And your job is going to be, you know, at the end of each line of code, there's a description of what the output means and you'll be collecting all of that output and then adding it here to to your section. Same with the results and the same with the analysis. So for the for the models and analyses. So on this part will be communicating with probably Verona to get the baseline model, what was the methodology and the results will summarize that in a few lines and then other models and results. Same with the statistical analysis and somebody else will be in charge of the discussion, which is a summary of our results. Learning points, the person who does the learning points is going to communicate with all sub teams and you're going to ask the subtask leaders, what limitations did you have, how did you overcome it, what do you suggest for the next, the next group of people that do this. So I think we need four or five leaders sub leaders to take these things on. Once we do that, I will work with each subgroup. So for example, for the data subgroup to fill out these sections. Okay, so where did we get our data source from WWW, whatever, who's the people behind it will say it's, you know, the NIH database. So we'll sort of start filling all of this up, but as bullet points. And then with the literature review that we gathered from here, I will start tagging in probably a star in a yellow highlight something like this and say lit review and it'll be for a particular point. So if we're talking about CNN models, okay, or other models, so we'll have a literature review here and we'll reference all the articles we found analysis analysis of models. Whether it be a systematic review or an actual study, and then we'll start fattening up this document. So I'm going to break this document up, but at this stage, I'll just pause my stop sharing the screen. At this stage, I'm just going to ask basically whoever's here, if anyone wants to volunteer for one or any other section. So let's start with the, we'll say introduction abstract and conclusion will be one team. Fair because it's a lot of the same things. I'll put that here in the chat intro abstract conclusion is one sub team. Methodology. Then discussion results. And then we have model modeling and data. Okay, so it's in the chat. Does anyone want to vote or volunteer to take on lead in any particular section. Hi, anyone, can you guys hear me? Yeah, we can show you. Okay, yes, we do. So who wants to anyone want to take lead? I'll be with you, by the way, I'll be doing the work alongside you each one group. It won't be like you're the leader and everything's on you. I'll be with you each group each stage each phase each sentence. But it's just so that we can start allocating which data goes to which section. That I can work with the modeling part. Maybe for now, I can just get the basic information. Along the way, we can gather more people into the research as well. Yep, let's let's make biology is in charge of modeling then. Yeah, like I won't be able to like a little contribute, but as long as like I have the right time like I I'll just keep in touch with whatever is happening with the modeling team and then. Give the information as we progress fine, so what will happen is I'll be with you okay, and yeah, yeah, modeling team and then we'll ask within the group. So I'm going to ask you to type in our group after who wants to support you and then. You can also delegate some tasks to other people in the group. Who you know will be behind you, but like I said, I'll be with you I'll I'll take on the primary and will support each other. Yeah, sure. Cool, so, but I do modeling section. Gary, I'm not anyone. Want to take on another section. It's not difficult work you guys it's very short term work your these sub sections we should be finished by I'm aiming for Saturday. So it's not you know detailed commitment and for the rest of the year or something. So I'll take them to the data part. Perfect, Shasha. Thank you so much. So we'll put you down for data. So you're going to lead basically a small sub team anyone else who wants to join the research aspect in terms of the data part. We'll kind of be under your guidance, but like I said, I'll be with you. Sure. Okay, amazing. Anyone for discussion result. Anyone there. Okay, methodology. No one. In so far as nobody volunteers, I'll take it on. And I'll mention now in our group, if there's anyone else wants to take that on. This is your chance to sort of take the lead. If you want. No cool. Okay, finished. So we'll keep it at Bellagy for modeling section. Shasha, I'm for data. I'll take on the rest and we'll do. You know, an option in the group for if anyone else wants to take on the other parts. Cool. Cool. All right. So I'm going to take the lead on discussion points. And I'll just take on those those that template that I was working on. I'm going to split it up into. Three papers now into three sections, Bellagy, you'll take on just the modeling part and Shasha and you'll start filling out the data part with me. Perfect. Cool. And like I said, you guys as the two sub leaders feel free to. To take the lead on discussion points in your subsection with other members of the group. You know, is interested in one of these parts and you can delegate tasks to them. Cool, cool. Alrighty. So I'll leave it at that now and you can expect for me then in the group. The various subsections. I'll share now the documents, the links and for us to start filling out. And that's it. I think we're good. Anyone have any questions or comments, suggestions, anything. Everyone's sleepy because it's Tuesday and midweek. All right. So I guess I'll end the call there and I'll set the next call for Saturday around the same time if it's okay. And we'll check in and see what progress we've made between now and Saturday. If it's cool. I know one question. Yes, please go ahead. I'm having trouble following the works happening. And so we are basically trying to summarize all the research papers, right? No, we're not summarizing anything. Those research papers in the table are going to be used as our go to. So that template that I showed you as we start filling out different sections. For example, in the baseline model, right? That was one of the subheadings. So, yeah, baseline model. If the baseline model used, you know, an A and N, let's say it used a recurrent neural network. So in that particular section, you would find the articles about recurrent neural networks that were used for autism diagnosis and just extract those key sentences that apply to that one part of our paper. But we don't we're not summarizing all the papers at all. We're just going to take from the papers. What we want to make our paper to suit the shape of our paper. Does that make sense? Yes. Thank you. And, oh, and I need, we need some gmails. So I need Bellagy and shashank's gmail addresses for short to give you admin access to that template. If you don't mind sending that to me, please. And anyone else who wants to take on, if you want to take on the methodology. Go, go, you don't want to try the methodology sections going to be pretty straightforward and easy. It's probably only going to be like a page long. Thanks for shank. I got it too scared. No, okay, no problem. It's all good. Well, I'll be with you. What if I did it alongside you, but this could be your chance to be a subtask leader and, you know, check how confident you can be. Denara, yeah. What, which part do you want to take on methodology? Sure. Hi. Sweet. Amazing. Cool. So that means we're going to get everyone. I'll just put it as a note. And I'll do it with you. Yeah. Excellent. I need your gmail as well then. Anyone who's going to be a subtasked just to give you admin sweet. What else do we have? So we have modeling data methodology intra abstract conclusion. I got it. So discussion results. Discussion results. We can leave it for now. I mean, we're not, we didn't arrive there. Malita, maybe. Lemya, you guys want to try. Shivan. Yeah, I can help with the work with discussion and conclusion. And the result section. So let's do it. Lemya and Malita are going to take on discussion results because there's going to be a lot of overlap. Okay, sure. Is that cool, Lemya? Sure. Sweet. Amazing. So discussion results. And I need your gmails, please. There we go. I can drop it here. Yeah, anywhere. Sure. Okay. I'll save this and then. So whoever sends me the gmail, I'm going to give you admin access to the template. You'll I'll maybe color coded or something so you know you can go to your relevant section. And we'll start filling it out together. I'll maybe write a prompt like add something here on this and then you can go to the Excel sheet. And you can go through this file. So that's a good thing. From an article. Skim through the article and take out what we need to fatten up that part of our template. Does that sound like a plan? So I think we could. Yeah. Do for Saturday, how about we just make it so by Saturday we explore and see where we got because it's you know new waters So you'll have a lot of exploration and you'll find a lot of things and interesting and should we include it should we not that's part of the fun So let's just do exploration and start tagging each other in that document for now until Saturday Kiroki Noted Well, and one last thing for the data and the modeling so shashank and belaji we're gonna have to get you in touch with the sub task leaders for the modeling team and for the data team You know in the general channel So I'll do an intro for you guys now and just make sure they add you to that team So you'll add as like an auditor, you know, you'll join that channel But to get an idea of what's going on and you ultimately need to take every all of their output and throw it into our document Yes, sure excellent great. So we have some g-mailing we have some Templating and we have some communicating to do with other teams between now and Saturday Oh, and it says rabbius will help with methodology. I just saw that so we'll do denara and rabbius in methodology I Think you guys could probably complete it just the two of you you might need anyone else But cool all right any other questions or we we call it a night All right cool I mean any questions Yeah, yeah I read little bit a little bit late. I was stuck in the traffic jam. So just could you just give quick Summarize of Today's a meeting so everybody will be on the same line I'll just reopen this At the beginning just in case anyone missed it. Right. So we had this This is the Excel and we've got a number of articles now and we've got the links plus The sort of notes so we know when the people who are doing the modeling section for example are likely going to access these papers Right because there's notes about modeling If if we end up using a data set that has imaging or eye gazing we're gonna likely need these articles So we're gonna use this as our go-to reference Bucket, you know to fill things up as needed and we're gonna fill it out in this template here So this is just a skeleton at the moment of the core components that should be in our paper, right the headings And some subheading so our job is gonna be to start filling this document out and Whoever is doing data will make a note about the source of the data the transparency the format the features Etc. At this stage. We're gonna fill it out in bullet points based on Data information we get either from other teams or from our own research Whoever is doing methodology Engineering feature analysis will get all this information from other teams and then combine it With what's been published out there as a citation support and that's it. We'll start filling this out as as is And that's Is that cool? Okay, excellent. So we've got our sub team Our accomplishments for today. We've completed our excel sheet We've delegated our sub team leaders. We're planning between now and Saturday to start Filling out that document and making the connections to the other groups So that we can be on the same page Okay Okay Cool, so have a good day if there's no other questions and we'll be In touch in the group and again feel free to reach out to me anytime for anything Okay, thank you, Dr. Shai. All right. Have a lovely day everybody Happy end of January Yeah Like everyone. Bye\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"Research_Team_Meeting_2_Jan31.wav\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38458fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d96ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyAudio\n",
      "  Downloading PyAudio-0.2.13-cp39-cp39-win_amd64.whl (164 kB)\n",
      "Installing collected packages: PyAudio\n",
      "Successfully installed PyAudio-0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22477be5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'wave' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2228/3030492626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'wave' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "print(wave.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76ff64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6102ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.4.5-py3-none-win_amd64.whl (195 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sounddevice) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.20)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520281c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from soundfile) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile) (2.20)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33459bc",
   "metadata": {},
   "source": [
    "# To record an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d3e4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ProjectGurukul's Voice recorder\n",
    "#Import necessary modules\n",
    "import sounddevice as sd\n",
    "from tkinter import *\n",
    "import queue\n",
    "import soundfile as sf\n",
    "import threading\n",
    "from tkinter import messagebox\n",
    "\n",
    "\n",
    "#Define the user interface\n",
    "voice_rec = Tk()\n",
    "voice_rec.geometry(\"360x200\")\n",
    "voice_rec.title(\"ProjectGurukul's Voice Recorder\")\n",
    "voice_rec.config(bg=\"#107dc2\")\n",
    "\n",
    "#Create a queue to contain the audio data\n",
    "q = queue.Queue()\n",
    "#Declare variables and initialise them\n",
    "recording = False\n",
    "file_exists = False    \n",
    "\n",
    "#Fit data into queue\n",
    "def callback(indata, frames, time, status):\n",
    "    q.put(indata.copy())\n",
    "\n",
    "#Functions to play, stop and record audio\n",
    "#The recording is done as a thread to prevent it being the main process\n",
    "def threading_rec(x):\n",
    "    if x == 1:\n",
    "        #If recording is selected, then the thread is activated\n",
    "        t1=threading.Thread(target= record_audio)\n",
    "        t1.start()\n",
    "    elif x == 2:\n",
    "        #To stop, set the flag to false\n",
    "        global recording\n",
    "        recording = False\n",
    "        messagebox.showinfo(message=\"Recording finished\")\n",
    "    elif x == 3:\n",
    "        #To play a recording, it must exist.\n",
    "        if file_exists:\n",
    "            #Read the recording if it exists and play it\n",
    "            data, fs = sf.read(\"trial.wav\", dtype='float32') \n",
    "            sd.play(data,fs)\n",
    "            sd.wait()\n",
    "        else:\n",
    "            #Display and error if none is found\n",
    "            messagebox.showerror(message=\"Record something to play\")\n",
    "\n",
    "#Recording function\n",
    "def record_audio():\n",
    "    #Declare global variables    \n",
    "    global recording \n",
    "    #Set to True to record\n",
    "    recording= True   \n",
    "    global file_exists \n",
    "    #Create a file to save the audio\n",
    "    messagebox.showinfo(message=\"Recording Audio. Speak into the mic\")\n",
    "    with sf.SoundFile(\"trial.wav\", mode='w', samplerate=44100,\n",
    "                        channels=2) as file:\n",
    "    #Create an input stream to record audio without a preset time\n",
    "            with sd.InputStream(samplerate=44100, channels=2, callback=callback):\n",
    "                while recording == True:\n",
    "                    #Set the variable to True to allow playing the audio later\n",
    "                    file_exists =True\n",
    "                    #write into file\n",
    "                    file.write(q.get())\n",
    "\n",
    "    \n",
    "#Label to display app title\n",
    "title_lbl  = Label(voice_rec, text=\"ProjectGurukul's Voice Recorder\", bg=\"#107dc2\").grid(row=0, column=0, columnspan=3)\n",
    "\n",
    "#Button to record audio\n",
    "record_btn = Button(voice_rec, text=\"Record Audio\", command=lambda m=1:threading_rec(m))\n",
    "#Stop button\n",
    "stop_btn = Button(voice_rec, text=\"Stop Recording\", command=lambda m=2:threading_rec(m))\n",
    "#Play button\n",
    "play_btn = Button(voice_rec, text=\"Play Recording\", command=lambda m=3:threading_rec(m))\n",
    "\n",
    "#Position buttons\n",
    "record_btn.grid(row=1,column=1)\n",
    "stop_btn.grid(row=1,column=0)\n",
    "play_btn.grid(row=1,column=2)\n",
    "voice_rec.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e53187",
   "metadata": {},
   "source": [
    "# Upload the Audio File If Stored Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d94674a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'upload_url': 'https://cdn.assemblyai.com/upload/7e7d6dd6-77c1-4b49-9e84-83663f46d561'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "filename = \"trial.wav\" # Give an url if its stored in cloud\n",
    "def read_file(filename, chunk_size=5242880):\n",
    "    with open(filename, 'rb') as _file:\n",
    "        while True:\n",
    "            data = _file.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data\n",
    "\n",
    "headers = {'authorization': \"1de1f4403c14482f82b6ef7e4217c90c\"}\n",
    "response = requests.post('https://api.assemblyai.com/v2/upload',\n",
    "                        headers=headers,\n",
    "                        data=read_file(filename))\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca7468",
   "metadata": {},
   "source": [
    "# Upload the Audio File If Stored In Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b1f8e83",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_30068/3939814080.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_30068/3939814080.py\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    print(response.json())\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "filename = 'Put S3 URL Here'\n",
    "\n",
    "headers = {'authorization': \"1de1f4403c14482f82b6ef7e4217c90c\"}\n",
    "response = requests.post('https://api.assemblyai.com/v2/upload',\n",
    "                        headers=headers,\n",
    "                        data=filename)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4290d1",
   "metadata": {},
   "source": [
    "# Store the uploaded URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0c112c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_url = response.json()['upload_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7316b23",
   "metadata": {},
   "source": [
    "# Send the upload file for Transcription & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68ca94d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'rd7gaatzwm-7377-4e48-a128-0a32319fb901', 'language_model': 'assemblyai_default', 'acoustic_model': 'assemblyai_default', 'language_code': 'en_us', 'status': 'queued', 'audio_url': 'https://cdn.assemblyai.com/upload/7e7d6dd6-77c1-4b49-9e84-83663f46d561', 'text': None, 'words': None, 'utterances': None, 'confidence': None, 'audio_duration': None, 'punctuate': True, 'format_text': True, 'dual_channel': None, 'webhook_url': None, 'webhook_status_code': None, 'webhook_auth': False, 'webhook_auth_header_name': None, 'speed_boost': False, 'auto_highlights_result': None, 'auto_highlights': False, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'content_safety': False, 'iab_categories': False, 'content_safety_labels': {}, 'iab_categories_result': {}, 'language_detection': False, 'custom_spelling': None, 'cluster_id': None, 'throttled': None, 'auto_chapters': False, 'summarization': False, 'summary_type': None, 'summary_model': None, 'disfluencies': False, 'sentiment_analysis': 'True', 'sentiment_analysis_results': None, 'chapters': None, 'entity_detection': False, 'entities': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
    "json = {\n",
    "    \"audio_url\": audio_url,\n",
    "    \"sentiment_analysis\": 'True'\n",
    "}\n",
    "headers = {\n",
    "    \"authorization\": \"1de1f4403c14482f82b6ef7e4217c90c\",\n",
    "}\n",
    "response = requests.post(endpoint, json=json, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232e5c6",
   "metadata": {},
   "source": [
    "# Check the status of the submitted request for Transcription & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af568014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'rd7gaatzwm-7377-4e48-a128-0a32319fb901', 'language_model': 'assemblyai_default', 'acoustic_model': 'assemblyai_default', 'language_code': 'en_us', 'status': 'completed', 'audio_url': 'https://cdn.assemblyai.com/upload/7e7d6dd6-77c1-4b49-9e84-83663f46d561', 'text': 'The event was really good because it showed all the work being done at Google. We had a lot of fun and hope to see more events like this in the future. Thank you.', 'words': [{'text': 'The', 'start': 570, 'end': 782, 'confidence': 0.89, 'speaker': None}, {'text': 'event', 'start': 836, 'end': 1150, 'confidence': 0.99911, 'speaker': None}, {'text': 'was', 'start': 1220, 'end': 1454, 'confidence': 0.99832, 'speaker': None}, {'text': 'really', 'start': 1492, 'end': 1790, 'confidence': 0.99961, 'speaker': None}, {'text': 'good', 'start': 1860, 'end': 2142, 'confidence': 0.99974, 'speaker': None}, {'text': 'because', 'start': 2196, 'end': 2414, 'confidence': 0.99966, 'speaker': None}, {'text': 'it', 'start': 2452, 'end': 2606, 'confidence': 0.9895, 'speaker': None}, {'text': 'showed', 'start': 2628, 'end': 2874, 'confidence': 0.57063, 'speaker': None}, {'text': 'all', 'start': 2922, 'end': 3086, 'confidence': 0.9988, 'speaker': None}, {'text': 'the', 'start': 3108, 'end': 3294, 'confidence': 0.97, 'speaker': None}, {'text': 'work', 'start': 3332, 'end': 3630, 'confidence': 0.99967, 'speaker': None}, {'text': 'being', 'start': 3700, 'end': 3982, 'confidence': 0.99957, 'speaker': None}, {'text': 'done', 'start': 4036, 'end': 4302, 'confidence': 0.99984, 'speaker': None}, {'text': 'at', 'start': 4356, 'end': 4814, 'confidence': 0.99, 'speaker': None}, {'text': 'Google.', 'start': 4932, 'end': 5598, 'confidence': 0.99988, 'speaker': None}, {'text': 'We', 'start': 5764, 'end': 6046, 'confidence': 0.99881, 'speaker': None}, {'text': 'had', 'start': 6068, 'end': 6206, 'confidence': 0.98861, 'speaker': None}, {'text': 'a', 'start': 6228, 'end': 6318, 'confidence': 0.99, 'speaker': None}, {'text': 'lot', 'start': 6324, 'end': 6494, 'confidence': 0.96129, 'speaker': None}, {'text': 'of', 'start': 6532, 'end': 6686, 'confidence': 1.0, 'speaker': None}, {'text': 'fun', 'start': 6708, 'end': 7134, 'confidence': 0.99995, 'speaker': None}, {'text': 'and', 'start': 7252, 'end': 7630, 'confidence': 0.6, 'speaker': None}, {'text': 'hope', 'start': 7700, 'end': 7934, 'confidence': 0.9988, 'speaker': None}, {'text': 'to', 'start': 7972, 'end': 8126, 'confidence': 0.99, 'speaker': None}, {'text': 'see', 'start': 8148, 'end': 8334, 'confidence': 0.9996, 'speaker': None}, {'text': 'more', 'start': 8372, 'end': 8574, 'confidence': 0.99973, 'speaker': None}, {'text': 'events', 'start': 8612, 'end': 8986, 'confidence': 0.96933, 'speaker': None}, {'text': 'like', 'start': 9018, 'end': 9166, 'confidence': 0.99925, 'speaker': None}, {'text': 'this', 'start': 9188, 'end': 9374, 'confidence': 0.9951, 'speaker': None}, {'text': 'in', 'start': 9412, 'end': 9566, 'confidence': 0.99717, 'speaker': None}, {'text': 'the', 'start': 9588, 'end': 9678, 'confidence': 0.99, 'speaker': None}, {'text': 'future.', 'start': 9684, 'end': 10142, 'confidence': 0.91618, 'speaker': None}, {'text': 'Thank', 'start': 10276, 'end': 10574, 'confidence': 0.90789, 'speaker': None}, {'text': 'you.', 'start': 10612, 'end': 10700, 'confidence': 0.99259, 'speaker': None}], 'utterances': None, 'confidence': 0.961724411764706, 'audio_duration': 12, 'punctuate': True, 'format_text': True, 'dual_channel': None, 'webhook_url': None, 'webhook_status_code': None, 'webhook_auth': False, 'webhook_auth_header_name': None, 'speed_boost': False, 'auto_highlights_result': None, 'auto_highlights': False, 'audio_start_from': None, 'audio_end_at': None, 'word_boost': [], 'boost_param': None, 'filter_profanity': False, 'redact_pii': False, 'redact_pii_audio': False, 'redact_pii_audio_quality': None, 'redact_pii_policies': None, 'redact_pii_sub': None, 'speaker_labels': False, 'content_safety': False, 'iab_categories': False, 'content_safety_labels': {'status': 'unavailable', 'results': [], 'summary': {}}, 'iab_categories_result': {'status': 'unavailable', 'results': [], 'summary': {}}, 'language_detection': False, 'custom_spelling': None, 'cluster_id': None, 'throttled': None, 'auto_chapters': False, 'summarization': False, 'summary_type': None, 'summary_model': None, 'disfluencies': False, 'sentiment_analysis': 'True', 'chapters': None, 'sentiment_analysis_results': [{'text': 'The event was really good because it showed all the work being done at Google.', 'start': 570, 'end': 5598, 'sentiment': 'POSITIVE', 'confidence': 0.9865146279335022, 'speaker': None}, {'text': 'We had a lot of fun and hope to see more events like this in the future.', 'start': 5764, 'end': 10142, 'sentiment': 'POSITIVE', 'confidence': 0.9889029860496521, 'speaker': None}, {'text': 'Thank you.', 'start': 10276, 'end': 10700, 'sentiment': 'POSITIVE', 'confidence': 0.8529649376869202, 'speaker': None}], 'entity_detection': False, 'entities': None, 'summary': None}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "endpoint = \"https://api.assemblyai.com/v2/transcript/\"+response.json()['id']\n",
    "headers = {\n",
    "    \"authorization\": \"1de1f4403c14482f82b6ef7e4217c90c\",\n",
    "}\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ead813",
   "metadata": {},
   "source": [
    "# Checking the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d797a141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'rd7gaatzwm-7377-4e48-a128-0a32319fb901',\n",
       " 'language_model': 'assemblyai_default',\n",
       " 'acoustic_model': 'assemblyai_default',\n",
       " 'language_code': 'en_us',\n",
       " 'status': 'completed',\n",
       " 'audio_url': 'https://cdn.assemblyai.com/upload/7e7d6dd6-77c1-4b49-9e84-83663f46d561',\n",
       " 'text': 'The event was really good because it showed all the work being done at Google. We had a lot of fun and hope to see more events like this in the future. Thank you.',\n",
       " 'words': [{'text': 'The',\n",
       "   'start': 570,\n",
       "   'end': 782,\n",
       "   'confidence': 0.89,\n",
       "   'speaker': None},\n",
       "  {'text': 'event',\n",
       "   'start': 836,\n",
       "   'end': 1150,\n",
       "   'confidence': 0.99911,\n",
       "   'speaker': None},\n",
       "  {'text': 'was',\n",
       "   'start': 1220,\n",
       "   'end': 1454,\n",
       "   'confidence': 0.99832,\n",
       "   'speaker': None},\n",
       "  {'text': 'really',\n",
       "   'start': 1492,\n",
       "   'end': 1790,\n",
       "   'confidence': 0.99961,\n",
       "   'speaker': None},\n",
       "  {'text': 'good',\n",
       "   'start': 1860,\n",
       "   'end': 2142,\n",
       "   'confidence': 0.99974,\n",
       "   'speaker': None},\n",
       "  {'text': 'because',\n",
       "   'start': 2196,\n",
       "   'end': 2414,\n",
       "   'confidence': 0.99966,\n",
       "   'speaker': None},\n",
       "  {'text': 'it',\n",
       "   'start': 2452,\n",
       "   'end': 2606,\n",
       "   'confidence': 0.9895,\n",
       "   'speaker': None},\n",
       "  {'text': 'showed',\n",
       "   'start': 2628,\n",
       "   'end': 2874,\n",
       "   'confidence': 0.57063,\n",
       "   'speaker': None},\n",
       "  {'text': 'all',\n",
       "   'start': 2922,\n",
       "   'end': 3086,\n",
       "   'confidence': 0.9988,\n",
       "   'speaker': None},\n",
       "  {'text': 'the',\n",
       "   'start': 3108,\n",
       "   'end': 3294,\n",
       "   'confidence': 0.97,\n",
       "   'speaker': None},\n",
       "  {'text': 'work',\n",
       "   'start': 3332,\n",
       "   'end': 3630,\n",
       "   'confidence': 0.99967,\n",
       "   'speaker': None},\n",
       "  {'text': 'being',\n",
       "   'start': 3700,\n",
       "   'end': 3982,\n",
       "   'confidence': 0.99957,\n",
       "   'speaker': None},\n",
       "  {'text': 'done',\n",
       "   'start': 4036,\n",
       "   'end': 4302,\n",
       "   'confidence': 0.99984,\n",
       "   'speaker': None},\n",
       "  {'text': 'at',\n",
       "   'start': 4356,\n",
       "   'end': 4814,\n",
       "   'confidence': 0.99,\n",
       "   'speaker': None},\n",
       "  {'text': 'Google.',\n",
       "   'start': 4932,\n",
       "   'end': 5598,\n",
       "   'confidence': 0.99988,\n",
       "   'speaker': None},\n",
       "  {'text': 'We',\n",
       "   'start': 5764,\n",
       "   'end': 6046,\n",
       "   'confidence': 0.99881,\n",
       "   'speaker': None},\n",
       "  {'text': 'had',\n",
       "   'start': 6068,\n",
       "   'end': 6206,\n",
       "   'confidence': 0.98861,\n",
       "   'speaker': None},\n",
       "  {'text': 'a',\n",
       "   'start': 6228,\n",
       "   'end': 6318,\n",
       "   'confidence': 0.99,\n",
       "   'speaker': None},\n",
       "  {'text': 'lot',\n",
       "   'start': 6324,\n",
       "   'end': 6494,\n",
       "   'confidence': 0.96129,\n",
       "   'speaker': None},\n",
       "  {'text': 'of',\n",
       "   'start': 6532,\n",
       "   'end': 6686,\n",
       "   'confidence': 1.0,\n",
       "   'speaker': None},\n",
       "  {'text': 'fun',\n",
       "   'start': 6708,\n",
       "   'end': 7134,\n",
       "   'confidence': 0.99995,\n",
       "   'speaker': None},\n",
       "  {'text': 'and',\n",
       "   'start': 7252,\n",
       "   'end': 7630,\n",
       "   'confidence': 0.6,\n",
       "   'speaker': None},\n",
       "  {'text': 'hope',\n",
       "   'start': 7700,\n",
       "   'end': 7934,\n",
       "   'confidence': 0.9988,\n",
       "   'speaker': None},\n",
       "  {'text': 'to',\n",
       "   'start': 7972,\n",
       "   'end': 8126,\n",
       "   'confidence': 0.99,\n",
       "   'speaker': None},\n",
       "  {'text': 'see',\n",
       "   'start': 8148,\n",
       "   'end': 8334,\n",
       "   'confidence': 0.9996,\n",
       "   'speaker': None},\n",
       "  {'text': 'more',\n",
       "   'start': 8372,\n",
       "   'end': 8574,\n",
       "   'confidence': 0.99973,\n",
       "   'speaker': None},\n",
       "  {'text': 'events',\n",
       "   'start': 8612,\n",
       "   'end': 8986,\n",
       "   'confidence': 0.96933,\n",
       "   'speaker': None},\n",
       "  {'text': 'like',\n",
       "   'start': 9018,\n",
       "   'end': 9166,\n",
       "   'confidence': 0.99925,\n",
       "   'speaker': None},\n",
       "  {'text': 'this',\n",
       "   'start': 9188,\n",
       "   'end': 9374,\n",
       "   'confidence': 0.9951,\n",
       "   'speaker': None},\n",
       "  {'text': 'in',\n",
       "   'start': 9412,\n",
       "   'end': 9566,\n",
       "   'confidence': 0.99717,\n",
       "   'speaker': None},\n",
       "  {'text': 'the',\n",
       "   'start': 9588,\n",
       "   'end': 9678,\n",
       "   'confidence': 0.99,\n",
       "   'speaker': None},\n",
       "  {'text': 'future.',\n",
       "   'start': 9684,\n",
       "   'end': 10142,\n",
       "   'confidence': 0.91618,\n",
       "   'speaker': None},\n",
       "  {'text': 'Thank',\n",
       "   'start': 10276,\n",
       "   'end': 10574,\n",
       "   'confidence': 0.90789,\n",
       "   'speaker': None},\n",
       "  {'text': 'you.',\n",
       "   'start': 10612,\n",
       "   'end': 10700,\n",
       "   'confidence': 0.99259,\n",
       "   'speaker': None}],\n",
       " 'utterances': None,\n",
       " 'confidence': 0.961724411764706,\n",
       " 'audio_duration': 12,\n",
       " 'punctuate': True,\n",
       " 'format_text': True,\n",
       " 'dual_channel': None,\n",
       " 'webhook_url': None,\n",
       " 'webhook_status_code': None,\n",
       " 'webhook_auth': False,\n",
       " 'webhook_auth_header_name': None,\n",
       " 'speed_boost': False,\n",
       " 'auto_highlights_result': None,\n",
       " 'auto_highlights': False,\n",
       " 'audio_start_from': None,\n",
       " 'audio_end_at': None,\n",
       " 'word_boost': [],\n",
       " 'boost_param': None,\n",
       " 'filter_profanity': False,\n",
       " 'redact_pii': False,\n",
       " 'redact_pii_audio': False,\n",
       " 'redact_pii_audio_quality': None,\n",
       " 'redact_pii_policies': None,\n",
       " 'redact_pii_sub': None,\n",
       " 'speaker_labels': False,\n",
       " 'content_safety': False,\n",
       " 'iab_categories': False,\n",
       " 'content_safety_labels': {'status': 'unavailable',\n",
       "  'results': [],\n",
       "  'summary': {}},\n",
       " 'iab_categories_result': {'status': 'unavailable',\n",
       "  'results': [],\n",
       "  'summary': {}},\n",
       " 'language_detection': False,\n",
       " 'custom_spelling': None,\n",
       " 'cluster_id': None,\n",
       " 'throttled': None,\n",
       " 'auto_chapters': False,\n",
       " 'summarization': False,\n",
       " 'summary_type': None,\n",
       " 'summary_model': None,\n",
       " 'disfluencies': False,\n",
       " 'sentiment_analysis': 'True',\n",
       " 'chapters': None,\n",
       " 'sentiment_analysis_results': [{'text': 'The event was really good because it showed all the work being done at Google.',\n",
       "   'start': 570,\n",
       "   'end': 5598,\n",
       "   'sentiment': 'POSITIVE',\n",
       "   'confidence': 0.9865146279335022,\n",
       "   'speaker': None},\n",
       "  {'text': 'We had a lot of fun and hope to see more events like this in the future.',\n",
       "   'start': 5764,\n",
       "   'end': 10142,\n",
       "   'sentiment': 'POSITIVE',\n",
       "   'confidence': 0.9889029860496521,\n",
       "   'speaker': None},\n",
       "  {'text': 'Thank you.',\n",
       "   'start': 10276,\n",
       "   'end': 10700,\n",
       "   'sentiment': 'POSITIVE',\n",
       "   'confidence': 0.8529649376869202,\n",
       "   'speaker': None}],\n",
       " 'entity_detection': False,\n",
       " 'entities': None,\n",
       " 'summary': None}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5064d71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The event was really good because it showed all the work being done at Google.',\n",
       "  'start': 570,\n",
       "  'end': 5598,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.9865146279335022,\n",
       "  'speaker': None},\n",
       " {'text': 'We had a lot of fun and hope to see more events like this in the future.',\n",
       "  'start': 5764,\n",
       "  'end': 10142,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.9889029860496521,\n",
       "  'speaker': None},\n",
       " {'text': 'Thank you.',\n",
       "  'start': 10276,\n",
       "  'end': 10700,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.8529649376869202,\n",
       "  'speaker': None}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['sentiment_analysis_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c27b2676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'The event was really good because it showed all the work being done at Google.',\n",
       "  'start': 570,\n",
       "  'end': 5598,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.9865146279335022,\n",
       "  'speaker': None},\n",
       " {'text': 'We had a lot of fun and hope to see more events like this in the future.',\n",
       "  'start': 5764,\n",
       "  'end': 10142,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.9889029860496521,\n",
       "  'speaker': None},\n",
       " {'text': 'Thank you.',\n",
       "  'start': 10276,\n",
       "  'end': 10700,\n",
       "  'sentiment': 'POSITIVE',\n",
       "  'confidence': 0.8529649376869202,\n",
       "  'speaker': None}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data = response.json()['sentiment_analysis_results']\n",
    "collected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c26fed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event was really good because it showed all the work being done at Google. :  POSITIVE\n",
      "We had a lot of fun and hope to see more events like this in the future. :  POSITIVE\n",
      "Thank you. :  POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment\n",
       "0  POSITIVE\n",
       "1  POSITIVE\n",
       "2  POSITIVE"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "speech_sentiment = []\n",
    "\n",
    "for sentence in collected_data:\n",
    "    print(sentence['text'],': ',sentence['sentiment'])\n",
    "    speech_sentiment.append(sentence['sentiment'])\n",
    "    \n",
    "temp_df = pd.DataFrame(data={\n",
    "    'Sentiment':speech_sentiment\n",
    "})\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a1adf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment:  POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print('Overall Sentiment: ',temp_df['Sentiment'].mode().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c16601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
